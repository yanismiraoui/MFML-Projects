{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN5AnuxQ/CaaQX04nk8oL3v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Gaussian Processes Regression\n","\n","<font color='green'> \n","In this notebook, we will implement a Gaussian Processes Regression model from scratch and apply it to solve practical problems. The main objectives are:\n","\n","1. Understanding how to fit a Gaussian Process Regression model.\n","2. Investigating how the kernel hyper-parameters can affect the model fitting.\n","3. Understanding how to select kernel hyper-parameters via *marginal loglikelihood optimisation*. \n","\n","</font>\n","\n","<font color='green'> \n","Complete the exercises marked with \"Exercise\". Fill in the code blanks marked with \"TODO\".\n","\n","Do NOT change the cells marked with \"TEST\" -- they are for testing your completed code.\n","</font>"],"metadata":{"id":"z0x0od4aPXcn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyJA2e8VDkAn"},"outputs":[],"source":["import autograd.numpy as np\n","from autograd import value_and_grad\n","import pandas as pd\n","import scipy\n","from scipy.optimize import minimize\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# Toy Data\n","\n","As a start, we will consider a synthetic dataset consisting of 3 data points of dimension 1, $X = \\{x_1, x_2, x_3 \\} = \\{ -1, 0, -1 \\}$, and the response variables $y = \\{y_1, y_2, y_3\\}$ are generated as follows:\n","\\begin{align*}\n","  y_i &= f(x_i) = \\frac{1}{1 + \\exp(- x_i)} - 1 \\;.\n","\\end{align*}"],"metadata":{"id":"jOD7CW0lFWgx"}},{"cell_type":"code","source":["def f(x):\n","  \"\"\"Data generating process.\"\"\"\n","  # return np.pi * x + 0.5\n","  return 1 / (1 + np.exp(- x)) - 1"],"metadata":{"id":"UnLm74gsTUfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# observed data\n","toy_X = np.array([-1., 0., 1.])\n","toy_y = f(toy_X)\n","\n","# plot observed data\n","plot_x = np.linspace(-5., 5., 100)\n","plt.plot(plot_x, f(plot_x))\n","plt.scatter(toy_X, toy_y, color=\"red\", s=70)"],"metadata":{"id":"6NOvMkpDQ2uq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Specifying a GP Prior\n","\n","Our aim is to approximate the function $f$ by fitting a Gaussian Process. That is, we place a GP prior on $f$ and seek the posterior distribution of $f$ conditional on the observed data. \n","\n","We will use a **noisy observations** model $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are i.i.d. Gaussian noise with variance $\\sigma^2$. Recall the GP model with mean function $m$ and kernel (covariance) function $K$ is,\n","$$\n","  \\mathbf{y} \\sim \\mathcal{N}(m(X), K(X, X)) \\;.\n","$$\n","Choosing a **zero function** for $m$, the posterior distribution at $M$ new test points $X^* = (x_1^*, \\ldots, x_M^*)$ is \n","\\begin{align*}\n","  f^* | X, \\mathbf{y}, X^* \n","  &\\sim \\mathcal{N}( \\tilde{m} ,\\; \\tilde{\\Sigma} ) \\;, \\\\\n","  \\tilde{m} &= K(X^*, X) [ K(X, X) + \\sigma^2 I_n ]^{-1} \\mathbf{y}, \\\\\n","  \\tilde{\\Sigma} &= K(X^*, X^*) - K(X^*, X) [K(X, X) + \\sigma^2 I_n]^{-1} K(X, X^*) ) \n","    ;.\n","\\end{align*}\n","\n","Remarks:\n","1. **Zero prior mean:** The choice of $m \\equiv 0$ is without loss of generality because we can always center the response variable by subtracting its empirical mean.\n","2. **Noisy observation model:** We could in principle use a noise-free observation model for this toy dataset. However, using the noisy model allows us to write the code in more generality, which will be helpful when we work with the real-life dataset in the second part of this notebook. Moreover, we will see later how to select the model hyper-parameters using a data-centric approach (i.e., log marginal likelihood maximisation). If the data are indeed generated from a noiseless process, then we should be able to recover the noise-free setting by selecting a very small $\\sigma_2$.\n","\n","For the **kernel function**, we will use the **Squared Exponential** kernel, also known as the **Radial Basis Function** (RBF) kernel. It is one of the most widely used choice of kernel function. Recall from the lecture notes that it is defined as\n","$$\n","k(x, x') = \\theta_1^2 \\exp\\left( - \\frac{1}{2 \\theta_2^2} \\| x - x' \\|_2^2 \\right) \\;,\n","$$\n","where $\\theta_1, \\theta_2 > 0$ are hyper-parameters that we need to specify. As we will see, the choice of these kernel hyper-parameters can have significant impact on the model quality. In practice, they are often chosen in a ''data-centric'' mannor, or by domain knowledge of the problem at hand. \n","\n","For now, we will set $\\theta_1, \\theta_2 = 1$.\n","\n","Also note that $d = 1$ in this example, so the $L_2$ norm reduces to a quadratic function. Throughout this notebook, we will use the $L_2$ norm in order to present everything in full generality, although **for the implementation you can assume the input is always one-dimensional**."],"metadata":{"id":"rlN9BdfaFq2p"}},{"cell_type":"markdown","source":["#### Exercise 1\n","Implement a RBF kernel by completing the code below.  Note that here both $x$ and $y$ denote the inputs, as opposed to the previous section where $y$ denotes the target.\n","\n","Your `RBF` class should \n","1. take as its input two rank-1 arrays of **potentially different** shapes, e.g., $x = [x_1, x_2, x_3] = [1., 2., 3.], y = [y_1, y_2] = [4., 5.]$.\n","2. output a kernel matrix with appropriate shape, e.g., k(x, y) is a $3 \\times 2$ array, where $k(x, y)_{i, j} = k(x_i, y_j)$.\n","3. **Bonus**: your code should use vectorised operations for fast computational speed."],"metadata":{"id":"Od_QKxKlJz6z"}},{"cell_type":"code","source":["class Kernel:\n","  def __init__(self):\n","    pass\n","\n","  def __call__(self, x, y, theta: list=None):\n","    raise NotImplementedError()\n","\n","class RBF(Kernel):\n","  def __call__(self, x, y, theta: list=None):\n","    \"\"\"\n","    Given an input array x of size n and another input array y of \n","    size m, compute the n by m kernel matrix whose (i, j)-th entry \n","    is k(x_i, y_j).\n","\n","    Args:\n","      x: (n,)\n","      y: (m,)\n","    \n","    Returns:\n","      k_mat: (n, m)\n","    \"\"\"\n","    # unpack hyper-params\n","    self.theta1, self.theta2 = theta\n","\n","    ### TODO ###\n","    k_mat = None\n","    ### END OF TODO ###\n","\n","    return k_mat"],"metadata":{"id":"Dud9b6gMHscQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TEST ###\n","rbf = RBF()\n","rbf(np.array([1., 2., 3.]), np.array([1., 2.]), theta=[1., 1.])\n","### END OF TEST ###"],"metadata":{"id":"hvJ_YT4LErpp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Expected output:\n","```python\n","array([[1.        , 0.60653066],\n","       [0.60653066, 1.        ],\n","       [0.13533528, 0.60653066]])\n","```"],"metadata":{"id":"9PVVNfY9Y8EZ"}},{"cell_type":"markdown","source":["## Gaussian Processes Regression\n","\n","We now implement a GP where the prior mean is assumed to be the **zero function**. The same logic will follow if an arbitrary mean function was used, except that the posterior mean and covariance will take a different form.\n"],"metadata":{"id":"j5O6NP0eKs67"}},{"cell_type":"markdown","source":["#### Exercise 2\n","Implement a Gaussian Process model by completing the `predict` method below. Your code should\n","1. Compute the posterior mean vector and covariance matrix for a given array of test points `x_star`.\n","2. Draw posterior samples for the predicted functional values at these test points.\n","3. Allow `x_star` to be a 1-rank array of arbitrary shape instead of only a scalar.\n","\n","**Note:** You do **not** need to complete the `marginal_loglik` method at this stage."],"metadata":{"id":"0We4-OC6llvj"}},{"cell_type":"code","source":["class GP:\n","  def __init__(self, kernel: callable):\n","    \"\"\"\n","    Args:\n","      kernel:\n","      prior_mean:\n","    \"\"\"\n","    self.k = kernel\n","\n","  def predict(\n","      self, \n","      x_star, \n","      X: np.array=None, \n","      y: np.array=None, \n","      size: int=1,\n","      theta: list=None,\n","      sigma: float=0.,\n","    ):\n","    \"\"\"\n","    Given observations (X, y) and test points x_star, fit a GP model\n","    and draw posterior samples for f(x_star) from the fitted model.\n","\n","    Args:\n","      x_star: (n*,) array of feature values at which predictions\n","        for f(x_star) will be made.\n","      X: (n,) observed features.\n","      y: (n,) observed response variables.\n","      size: number of posterior samples drawn.\n","      theta: (n_hyperparams,) array of kernel hyper-parameters.\n","\n","    Returns:\n","      y_star: (size, n*) array of posterior samples for f(y_star).\n","    \"\"\"\n","    ### TODO ###\n","    # 1. compute \n","    # - k(x*, X)\n","    # - k(X, x*)\n","    # - k(x*, x*)\n","    # - k(X, X) + sigma^2 I_n\n","    k_xs_x = None\n","    k_x_xs = None\n","    k_xs_xs = None\n","    k_x_x = None\n","    cov_x_x = None\n","    ### END OF TODO ###\n","\n","    ### TODO ###\n","    # 2. compute posterior means and covariance matrix\n","    posterior_mean = None\n","    posterior_var = None\n","\n","    ### END OF TODO ###\n","\n","    self.posterior_mean = posterior_mean\n","    self.posterior_var = posterior_var\n","\n","    ### TODO ###\n","    # 3. draw posterior samples by using the posterior\n","    # mean and covariance matrix\n","    y_star = None\n","    \n","    ### END OF TODO ###\n","\n","    return y_star\n","\n","  def marginal_loglik(self, X, y, theta: list=None, sigma: float=0.):\n","    \"\"\"\n","    Given observations (X, y), compute the marginal loglikelihood\n","    of a GP model.\n","\n","    Args:\n","      X: (n,) observed features.\n","      y: (n,) observed response variables.\n","      theta: (n_hyperparams,) array of kernel hyper-parameters.\n","\n","    Returns:\n","      log_lik: marginal loglikelihood at (X, y).\n","    \"\"\"\n","    ### TODO ###\n","    k_x_x = None\n","    cov_x_x = None\n","\n","    data_fit = None\n","    penalty = None\n","\n","    log_lik = data_fit + penalty\n","\n","    ### END OF TODO ###\n","\n","    return log_lik"],"metadata":{"id":"CGgglqwCJ9l1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" We now fit a GP model on our observations. For the hyper-parameters $\\theta = (\\theta_1, \\theta_2)$ of the RBF kernel, we choose arbitrarily $\\theta_1 = \\theta_2 = 1$. We will discuss the effect of different values on the fitted result in the next section.\n","\n","#### Exercise 3\n","Fit a GP model on the observed data and draw 100 posterior samples from the fitted model.\n","1. Initialise the test points to be 50 equally spaced between -5 and 5.\n","2. Set the kernel hyper-parameter to be $\\theta = (1, 1)$. \n","3. Set the noise std to be $\\sigma = 0.1$."],"metadata":{"id":"oyUqcQMIhA6E"}},{"cell_type":"code","source":["# instantiate kernel and GP model\n","rbf = RBF()\n","gp = GP(kernel=rbf)\n","\n","### TODO ###\n","# initialise test points\n","toy_xp = None\n","\n","# choose kernel hyper-params\n","theta = None\n","sigma = None\n","\n","### END OF TODO ###"],"metadata":{"id":"g6QEpKRiJewN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# draw 100 posterior predictions\n","# set seed for reproducibility\n","np.random.seed(1)\n","### TODO ###\n","yp = gp.predict(\n","  x_star=None,  # set to test points\n","  X=None,       # set to observed x\n","  y=None,       # set to observed y\n","  size=None,    # draw 100 posterior samples \n","  theta=theta,\n",")\n","### END OF TODO ###"],"metadata":{"id":"KamJSwmTJe5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot posterior draws and observations\n","fig = plt.figure()\n","plt.plot(toy_xp, np.transpose(yp), color=\"grey\", alpha=0.2)\n","plt.scatter(toy_X, toy_y, color=\"red\", s=70)"],"metadata":{"id":"qlMFYrEV8Xun"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Effect of Kernel Hyper-Parameters\n","\n","The choice of the kernel hyper-parameters can often have significant consequences on the fitted GP. To illustrate this with the RBF kernel, we choose $\\theta_2 = 1$ and vary $\\theta_1$, and plot posterior samples drawn from the GP fitted with these hyper-parameters. "],"metadata":{"id":"HDXdmbErAy64"}},{"cell_type":"code","source":["def plot_varying_theta1(theta1_list, theta2=1.):\n","  \"\"\"\n","  Plot posterior draws from GPs fitted with different theta1 and\n","  fixed theta2.\n","  \"\"\"\n","  nrow = 2\n","  ncol = 3\n","  figs, axes = plt.subplots(nrows=nrow, ncols=ncol, figsize=(12, 8))\n","\n","  for i, theta1 in enumerate(theta1_list):\n","    rbf = RBF()\n","    gp = GP(kernel=rbf)\n","    yp = gp.predict(\n","        x_star=toy_xp, X=toy_X, y=toy_y, size=100, theta=[theta1, 1.], theta=0.1,\n","    )\n","\n","    axes[i // ncol][i % ncol].plot(toy_xp, np.transpose(yp), color=\"grey\", alpha=0.2)\n","    axes[i // ncol][i % ncol].scatter(toy_X, toy_y, color=\"red\", s=70)\n","    axes[i // ncol][i % ncol].set_xlim(-5., 5.)\n","    axes[i // ncol][i % ncol].set_ylim(-20., 20.)\n","    axes[i // ncol][i % ncol].set_title(f\"theta1 = {theta1}\")\n","\n","  plt.tight_layout()\n","\n","\n","# vary theta1, fix theta2\n","theta1_list = np.array([0.1, 0.5, 1., 5., 10., 50.])\n","plot_varying_theta1(theta1_list)"],"metadata":{"id":"qc17_4C38XkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Exercise 4\n","Repeat the above analysis by **fixing $\\theta_1$** and **varying $\\theta_2$**.\n","- You are free to choose the location of the test points."],"metadata":{"id":"UXoonBxqil51"}},{"cell_type":"code","source":["def plot_varying_theta1(theta2_list, theta1=1.):\n","  \"\"\"\n","  Plot posterior draws from GPs fitted with fixed theta1 and\n","  different theta2.\n","  \"\"\"\n","  nrow = 2\n","  ncol = 3\n","  figs, axes = plt.subplots(nrows=nrow, ncols=ncol, figsize=(12, 8))\n","\n","  for i, theta2 in enumerate(theta1_list):\n","    ### TODO ###\n","    rbf = None\n","    gp = None\n","    yp = None\n","    ### END OF TODO ###\n","\n","    axes[i // ncol][i % ncol].plot(toy_xp, np.transpose(yp), color=\"grey\", alpha=0.2)\n","    axes[i // ncol][i % ncol].scatter(toy_X, toy_y, color=\"red\", s=70)\n","    axes[i // ncol][i % ncol].set_xlim(-5., 5.)\n","    axes[i // ncol][i % ncol].set_ylim(-10., 10.)\n","    axes[i // ncol][i % ncol].set_title(f\"theta2 = {theta2}\")\n","\n","  plt.tight_layout()\n","\n","\n","### TODO ###\n","# vary theta1, fix theta2\n","theta2_list = None\n","### END OF TODO ###\n","\n","plot_varying_theta1(theta2_list)"],"metadata":{"id":"kc0Bm6M7Cmg6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Exercise 5\n","Comment on how these two hyper-parameters affect the posterior samples.\n","\n","<details><summary>Hint</summary>\n","<p>\n","\n","Provide mathematical interpretations by assuming $\\sigma = 0$ (i.e., a **noise-free** model) and arguing how the posterior mean and covariance matrix change with $\\theta_1$ and $\\theta_2$.\n","\n","</p>\n","</details>\n","\n"],"metadata":{"id":"AHQ8reMQDa2O"}},{"cell_type":"markdown","source":["<details><summary>Solution</summary>\n","<p>\n","\n","<font color=\"green\"> **$\\theta_1$ is the magnitude and controls the \"variability\" of the posterior functions** </font>. For a mathematical interpretation, let us assume the noise std $\\sigma$ small so that we are in the **noise-free** setting. Recall that the posterior GP then takes the form\n","\\begin{align*}\n","  f^* | f, X, X^* &\\sim \\mathcal{N}( \\tilde{m} ,\\; \\tilde{\\Sigma} )\n","  \\;, \\\\\n","  \\tilde{m} &= K(X^*, X) K(X, X)^{-1} f \\\\\n","  \\tilde{\\Sigma} &= K(X^*,X^*)-K(X^*,X)K(X,X)^{-1}K(X,X^*) \\;.\n","\\end{align*}\n","Denote by $k_\\theta(x, x')$ the RBF kernel with hyper-parameter $\\theta = (\\theta_1, \\theta_2)$, and similarly define the corresponding covariance matrix $K_\\theta$. Also denote by $\\tilde{m}_\\theta, \\tilde{\\Sigma}_\\theta$ the posterior mean and covariance using kernel $K_\\theta$. It is straghtforward to see that $k_{(\\theta_1, \\theta_2)}(x, x') = \\theta_1 k_{(1, \\theta_2)} (x, x')$ for any $\\theta_1, \\theta_2 > 0$. Therefore,\n","- The posterior mean does **not** depend on $\\theta_1$ due to cancellation.\n","- The posterior covariance function scales linearly with $\\theta_1$:\n","$$\n","  \\tilde{\\Sigma}_\\theta\n","  = \\theta_1 K_{(1, \\theta_2)}(X^*,X^*) - \\theta_1 K_{(1, \\theta_2)}(X^*,X)K_{(1, \\theta_2)}(X,X)^{-1}K_{(1, \\theta_2)}(X,X^*) \n","  = \\theta_1 \\tilde{\\Sigma}_{(1, \\theta_2)}\n","  \\;.\n","$$\n","\n","Hence, increasing $\\theta_1$ does not change the posterior mean but increases the posterior variance. This is evident from the first plot.\n","\n","<font color=\"green\"> **$\\theta_2$ is the length-scale and controls the \"wigglyness\" of the posterior functions** </font>. Given $x, x' \\in \\mathbb{R}^d$, if $x = x'$, then the kernel value is $\\theta_1$. We suppose $x \\neq x'$. If $\\theta_2 \\ll \\| x - x' \\|_2$, then the exponent in the RBF kernel will be negatively large, so $k_\\theta(x, x')$ will be close to 0. Therefore, the kernel matrix will tend to $\\theta_1 I_d$ as $\\theta_2$ increases, where $I_d$ is the $d \\times d$ identity matrix. It follows that the posterior prediction will tend to a **standard Gaussian distribution**. This is illustrated in the top row of the second plot, where for $\\theta_2$ small, the predictions seem to be simply random noises.\n","\n","On the other hand, if $\\theta_2 \\gg \\| x - x' \\|_2$, then $k_\\theta(x, x')$ will be close to $\\theta_1$ for all $x \\neq x'$, and the kernel matrix will tend to a **degenerate** matrix where each entry equals to $\\theta_1$. This suggests a almost perfect correlation between the predicted values at every test point. This is illustrated by the bottom row of the second plot, where for $\\theta_2$ large the posterior samples have almost no variation amongst its posterior mean.\n","\n","In general, a large $\\theta_2$ is suitable for capturing **long-term** trend in $x$, whilst a small $\\theta_2$ is often used for modelling **short-term** effects.\n","\n","<p>\n","</details>\n"],"metadata":{"id":"SqaUPp64kFjK"}},{"cell_type":"markdown","source":["## Model Selection\n","One famous approach to optimise the hyper-parameters in a GP model is by maximising the so-called **marginal likelihood** (also known as **evidence**), $p(y | X)$. It is defined as the following integral of the likelihood multiplied by the prior:\n","$$\n","  p(y | X) = \\int p(y | f, X) p(f | X) df \\;.\n","$$\n","The term **marginal** refers to the marginalisation over the functions $f$. Under a GP model, the prior is Gaussian, $f | X \\sim \\mathcal{N}(0, K)$, so\n","$$\n","  p(f | X) = -\\frac{1}{2} f^\\top K(X, X)^{-1} f - \\frac{1}{2} \\log | K(X, X) | - \\frac{n}{2} \\log2 \\pi \\;.\n","$$\n","Under the noisy observation assumption, $y_i = f(x_i) + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, so the likelihood is a factorised Gaussian $y | f, X \\sim \\mathcal{N}(f(X), \\sigma^2 I_n)$. \n","\n","Using the fact that the product of Gaussians is still a Gaussian (e.g., Section 4.2.3 of the lecture notes), we conclude (**exercise**):\n","\\begin{align*}\n","  \\log p(y | X) = - \\frac{1}{2} y^\\top (K(X, X) + \\sigma^2 I_n)^{-1} y - \\frac{1}{2} \\log | K(X, X) + \\sigma^2 I_n | - \\frac{n}{2} \\log 2\\pi \\;.\n","\\end{align*}\n","We call the above equation the **log marginal likelihood**. \n","<!-- To emphasize its dependence on the hyper-parameters $\\theta$ and $\\sigma$, we also write $\\log p(y | X, \\theta, \\sigma)$.  -->\n","\n","The three terms here have intuitive roles:\n","- $- \\frac{1}{2} y^\\top (K(X, X) + \\sigma^2 I_n)^{-1} y$ is the **data fit** and is the only term involving the observed targets $y$.\n","- $\\log | K(X, X) + \\sigma^2 I_n |$ is the **complexity penalty** that depends only on the input $X$ and kernel fuction.\n","- $\\frac{n}{2} \\log 2\\pi$ is a **normalising constant** that does not depend on the hyper-parameters $\\theta$ nor $\\sigma$, so it can be ignored when performing optimisation. \n"],"metadata":{"id":"ehKsuPrOHAzj"}},{"cell_type":"markdown","source":["#### Exercise 6\n","Implement the log marginal likelihood by completing the `marginal_loglik` method in the `GP` class defined in the previous section.\n","- Your code should be numerically stable. You might find `numpy.linalg.slogdet` useful.\n","\n","> **Remark:** Remember to re-run all previous cells to make sure the new `GP` class is used for the remaining of this notebook.  "],"metadata":{"id":"xkCNByHax3vi"}},{"cell_type":"code","source":["rbf = RBF()\n","gp = GP(kernel=rbf)"],"metadata":{"id":"UVdpyP8TCmZO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TEST ###\n","gp.marginal_loglik(X=toy_X, y=toy_y, theta=theta, sigma=sigma)\n","### END OF TEST ###"],"metadata":{"id":"yHjVJiZyCmQL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Expected output:\n","```python\n","0.2189792097792156\n","\n","```"],"metadata":{"id":"YzSMzpV_XIPf"}},{"cell_type":"markdown","source":["To find the best hyper-parameters, we seek the **partial derivatives** of the marginal loglikelihood with respect to $\\theta$ and $\\sigma$, i.e., $\\frac{\\partial}{\\partial \\theta_j} \\log p(y | X) $ and $\\frac{\\partial}{\\partial \\sigma} \\log p(y | X) $. This can be done by either manually computing these derivatives, or using **auto-differentiation**.\n","\n","In this notebook, we will use [**auto-differentiation**](https://en.wikipedia.org/wiki/Automatic_differentiation) and an optimisation algorithm called [**conjugate gradient (CG)** method](https://en.wikipedia.org/wiki/Conjugate_gradient_method) to perform the hyper-parameter search. You do not need to know how auto-differentiation and CG method work for this notebook, but you are encouraged to look it up if it interests you. Also, any other optimisation algorithm (e.g., gradient descent) will indeed work too.\n","\n","We use the [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) API to perform the optimisation for both $\\theta$ and $\\sigma$. Since our aim is to **maximise** the log marginal likelihood, we will set the objective function to be the **negative log marginal likelihood**. We will also use `value_and_grad` from the [`autograd`](https://github.com/HIPS/autograd) library, which returns both the objective function and its gradient function. "],"metadata":{"id":"Lr6q2eQyyZhZ"}},{"cell_type":"code","source":["def optim_hyperparams(init_params, data_X, data_y, gp, method=\"CG\", maxiter=60):\n","  \"\"\"\n","  Find the best kernel hyper-parameters that maximise the log marginal\n","  likelihood.\n","  \"\"\"\n","  # define negative log marginal likelihood as objective\n","  # input is unpacked to theta and sigma\n","  objective = lambda params: -gp.marginal_loglik(\n","    data_X, data_y, theta=params[:-1], sigma=params[-1],\n","  )\n","\n","  optim_res = minimize(\n","      fun=value_and_grad(objective),\n","      jac=True,\n","      x0=init_params, \n","      method=method,\n","      options={\"return_all\": True, \"maxiter\": maxiter},\n","  )\n","  return optim_res"],"metadata":{"id":"pwQxYk7-VBuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We set the initial set of values for the optimiser to be $\\theta^{(0)} = (1, 1)$ and $\\sigma^{(0)} = 0.1$. Note that the CG method (and in general most optimisation algorithms) can be **very sensitive** to the starting point."],"metadata":{"id":"A6_1ARoYs88g"}},{"cell_type":"code","source":["init_params = [1., 1., 0.1]\n","otpim_res = optim_hyperparams(init_params=init_params, data_X=toy_X, data_y=toy_y, gp=gp)\n","otpim_res"],"metadata":{"id":"okUR4gF-WCtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_optim_history(params_history, gp, X, y):\n","  \"\"\"\n","  Plotting util for log marginal likelihood against iterations.\n","  \"\"\"\n","  log_lik_history = []\n","  for params in params_history:\n","    log_lik = gp.marginal_loglik(X=X, y=y, theta=params[:-1], sigma=params[-1])\n","    log_lik_history.append(log_lik)\n","\n","  plt.plot(log_lik_history)\n","  plt.xlabel(\"Iteraitons\")\n","  plt.ylabel(\"Log marginal likelihood\")\n","  plt.show()\n","  return log_lik_history"],"metadata":{"id":"3t4E3dhWSJfo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_ = show_optim_history(otpim_res.allvecs, gp, toy_X, toy_y)"],"metadata":{"id":"GG0QBnDFSJm2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mauna Loa Atmospheric $\\textrm{CO}_2$ Data\n","\n","In the second part of this notebook, we will apply our GP model to a real-life dataset concerning the concentration of $\\textrm{CO}_2$. The [Mauna Loa Atmospheric $\\textrm{CO}_2$ dataset](https://gml.noaa.gov/ccgg/trends/) consists of concentration of $\\textrm{CO}_2$  (in parts per million by volume (ppm)) from the Mauna Loa Observatory from 1958 to 2022. We are interesting in modelling this concentration against time and extrapolating it to future years.\n","\n","The input $x$ is the observation time (`decimal date`) and the observed target $y$ is the monthly average concentration level (`average`). We aim to fit a **noisy** GP regression model, as we expect the targets are corrupted by noises, potentially due to, e.g., observational error or measurement inaccuracies.\n","\n","<!-- Note that since this is a time-series data, we expect the observed targets to be **correlated** given test inputs. This renders  -->"],"metadata":{"id":"3o5DJnglT8n9"}},{"cell_type":"markdown","source":["## Loading Data"],"metadata":{"id":"ZLUoY6B0H9ca"}},{"cell_type":"code","source":["# download data\n","url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\"\n","df = pd.read_csv(url, skiprows=52) # skip the first 52 rows containing comments\n","df.head()"],"metadata":{"id":"NSqkVYM9H9m-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.plot(x=\"decimal date\", y=\"average\")"],"metadata":{"id":"gM6N0p2kH9nA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<!-- #### Exercise -->\n","\n","We center the observed target by subtracting its empirical mean. "],"metadata":{"id":"UIcvfxjnbtpM"}},{"cell_type":"code","source":["# convert data to numpy array\n","data_X = df[\"decimal date\"].to_numpy()\n","data_y = df[\"average\"].to_numpy()\n","\n","# substract empirical mean\n","data_y -= np.mean(data_y)"],"metadata":{"id":"Zlrca1tsH9nB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Specifying Kernel Function\n","We choose as the kernel function a sum of 4 kernel functions that are able to capture different characteristics of the data (recall that sums and products of kernel functions are also kernel functions). Note that we are not claiming this is the best kernel function -- it is entirely possible that other choices are more appropriate for this problem.\n","\n","1. To model the long-term smooth trend, we use a RBF kernel \n","$$\n","k_1(x, x') = \\theta_1^2 \\exp\\left( - \\frac{\\| x - x' \\|_2^2}{2\\theta_2^2} \\right) \\;.\n","$$\n","\n","2. To model the periodic behaviour, we use a product of a RBF kernel and a **periodic kernel**, taking the form\n","$$\n","k_2(x, x') = \\theta_3^2 \\exp\\left( - \\frac{\\| x - x' \\|_2^2}{2 \\theta_4^2} - \\frac{2 \\sin^2( \\pi (x - x') )}{\\theta_5^2} \\right) \\;,\n","$$\n","where $\\theta_3$ represents the magnitude, $\\theta_4$ is the **decay-time** for the periodic component, and $\\theta_5$ gives its smoothness. The period (in the argument of $\\sin$) has been set to one (year). This is because the seasonal effect is believed to be caused primarily by the different $\\textrm{CO}_2$ level uptake for plants depending on the season, although in principle we could have introduced another hyper-parameter to model it completely from the data. \n","\n","3. To model the small/medium-term irregularities, we use a **rational quadratic kernel**\n","$$\n","k_3(x, x') = \\theta_6^2 \\left( 1 + \\frac{\\| x - x' \\|_2^2}{2 \\theta_7^2 \\theta_8} \\right)^{- \\theta_8} \\;,\n","$$\n","where $\\theta_6$ gives the magnitude, $\\theta_7$ the standard length-scale, and $\\theta_8$ the shape parameter. One could have used a RBF kernel for this component. However, since we are interested in modelling the **small/medium** effect, a rational quadratic kernel is a better choice. This is because this kernel decays **polynomially** as a function of $\\| x - x'\\|_2^2$ (controlled by $\\theta_8$), whereas the decay in RBF is exponentially and therefore much faster. This means a rational quadratic kernel can accommodate several length-scales better than a RBF can do.\n","\n","4. To model the noise term, we use a sum of correlated and uncorrelated components\n","$$\n","k_4(x_p, x_q) = \\theta_9^2 \\exp\\left( - \\frac{\\| x_p - x_q \\|_2^2}{2\\theta_{10}^2} \\right) + \\theta_{11}^2 \\delta_{pq} \\;,\n","$$\n","where $\\theta_9$ is the magnitude of the correlated component, $\\theta_{10}$ the length-scale, and $\\theta_{11}$ the magnitude of the uncorrelated component. The correlated term is introduced because the data is a time-series that is believed to suffer from systematic noises (measurement inaccuracies, short-term weather phenomenon etc.). The uncorrelated term served exactly the same role as the noise term $\\sigma^2 \\delta_{pq}$ in a noisy GP model except a renaming. Therefore, when instantiating $k_4$ in our code we could equivalently only create a RBF kernel.\n","\n","One may also wonder whether the first component in $k_4$ is redundant as it has an identical form as $k_1$. The answer is no -- we will see that after hyper-parameter optimisation one will have a **short length-scale (noise)** and the other a **long length-scale (the long-term signal)**. These two terms are, however, **unidentifiable**, meaning that we may observe $k_1$ to have a long length-scale and the first term in $k_4$ to have a short length-scale, and vice versa. It is only a matter of interpretation that we have chosen to call one the \"noise\" term and the other the \"signal\".\n","\n","These choices are made based on our prior understanding of the data (e.g., by inspecting the data plot). This is often how the kernel function for GPs are consructed in practice, where the practitioner choose a combination of kernels that can model different characteristics of the data. For example, the periodic kernel is used if the data exhibit periodic trend, and the RBF kernel is often the go-to choice to model smooth functions. A survey of well-known kernel functions can be found on this [kernel cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/).\n","\n"],"metadata":{"id":"giiXzv-mIBo9"}},{"cell_type":"code","source":["class Periodic(Kernel):\n","  def __call__(self, x, y, theta: list=None):\n","    \"\"\"\n","    Compute the periodic kernel matrix.\n","\n","    Args:\n","      x: (n,)\n","      y: (m,)\n","    \n","    Returns:\n","      k_mat: (n, m)\n","    \"\"\"\n","    theta3, theta4, theta5 = theta\n","\n","    ### TODO ###\n","    k_mat = None\n","\n","    ### END OF TODO ###\n","\n","    return k_mat"],"metadata":{"id":"HWoYepS7bU-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RationalQuadratic(Kernel):\n","  def __call__(self, x, y, theta: list=None):\n","    \"\"\"\n","    Compute the rational quadratic kernel matrix.\n","\n","    Args:\n","      x: (n,)\n","      y: (m,)\n","    \n","    Returns:\n","      k_mat: (n, m)\n","    \"\"\"\n","    # unpack hyper-params\n","    theta6, theta7, theta8 = theta\n","\n","    ### TODO ###\n","    k_mat = None\n","\n","    ### END OF TODO ###\n","\n","    return k_mat"],"metadata":{"id":"2JAi8KipcSu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Sum(Kernel):\n","  def __init__(self):\n","    super().__init__()\n","    self.rbf = RBF()\n","    self.periodic = Periodic()\n","    self.rq = RationalQuadratic()\n","\n","  def __call__(self, x, y, theta: list=None):\n","    \"\"\"\n","    Compute the kernel matrix k = k_1 + k_2 + k_3 + k_4.\n","\n","    Args:\n","      x: (n,)\n","      y: (m,)\n","    \n","    Returns:\n","      k_mat: (n, m)\n","    \"\"\"\n","    k_mat = (\n","        self.rbf(x, y, theta[:2]) +\n","        self.periodic(x, y, theta[2:5]) +\n","        self.rq(x, y, theta[5:8]) +\n","        self.rbf(x, y, theta[8:])\n","    )\n","    return k_mat"],"metadata":{"id":"2VZ1LEbCdDcs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fitting the Model\n","\n","Let us fit a GP regression model with a set of arbitrarily chosen kernel hyper-parameters."],"metadata":{"id":"1wbdUFk6z6vI"}},{"cell_type":"code","source":["sum_kernel = Sum()\n","gp = GP(kernel=sum_kernel)"],"metadata":{"id":"4s2_7UM6T8fB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Exercise 7\n","Generate 100 posterior predictions for the concentration level for the future 20 years. \n","1. Initialise test time `pred_X` to be an array from the last time point in `data_X` until year 2043 (including both end points), equally spaced with step 1/12. \n","2. Set all hyper-parameter values to be $1$.\n"],"metadata":{"id":"37cpDluVeKJ_"}},{"cell_type":"code","source":["### TODO ###\n","# predict 20 years into the future\n","pred_X = None\n","pred_y = None\n","### END OF TODO ###"],"metadata":{"id":"CGUWLLhcYUho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_predicted_co2(data_X, data_y, pred_X, pred_y):\n","  \"\"\"\n","  Plotting util for CO2 predictions.\n","  \"\"\"\n","  fig = plt.figure()\n","  _ = plt.plot(data_X, data_y, label=\"observed\")\n","  _ = plt.plot(pred_X, np.transpose(pred_y), label=\"predicted\", color=\"cyan\", alpha=0.2)\n","  _ = plt.xlabel(\"Year\")\n","  _ = plt.ylabel(\"Predicted concentration (centred)\")"],"metadata":{"id":"bujpSHqjs8o-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# posterior predictions\n","_ = plot_predicted_co2(data_X, data_y, pred_X, pred_y)"],"metadata":{"id":"LvB6fu04tLGk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us now optimise the kernel hyper-parameters to improve the model fitting.\n","\n","The final hyper-parameter values above are only a **local** optima, since the objective function is highly likely to be **multi-modal** (it is defined on a 11-dimensional space!). One way to accommodate this is to **try multiple starting points** for the optimisation algorithm, and select the one that gives the highest log marginal likelihood.\n","\n","We use 10 sets of initial values, and find the best hyper-parameters in the 10 runs. We could randomly generate these values from some given intervals, as usually done in practice. However, this will take an long time. For the sake of this tutorial, we generate initial values **close to a set of local optima** reported in [1]."],"metadata":{"id":"-EV_b6SBRotO"}},{"cell_type":"code","source":["def optim_hyperparams_multiple_runs(init_params_list, data_X, data_y, gp, maxiter=60):\n","  \"\"\"\n","  Run hyper-parameter search with multiple starting points.\n","  \"\"\"\n","  optim_res_list = []\n","  log_lik_history_list = []\n","  nrep = len(init_params_list)\n","\n","  for i in range(nrep):\n","    print(f\"iter {i+1} / {nrep}\")\n","    init_params = init_params_list[i]\n","\n","    # find best params\n","    optim_res = optim_hyperparams(\n","        init_params, data_X, data_y, gp, method=\"CG\", maxiter=maxiter,\n","    )\n","    # log_lik_history = show_optim_history(optim_res.allvecs, gp, data_X, data_y)\n","    params = optim_res.allvecs[-1]\n","    # compute log marginal lkhd\n","    log_lik = gp.marginal_loglik(data_X, data_y, theta=params[:-1], sigma=params[-1])\n","\n","    # store results\n","    optim_res_list.append(optim_res)\n","    log_lik_history_list.append(log_lik)\n","\n","  return optim_res_list, log_lik_history_list "],"metadata":{"id":"ZqfMTkNAONyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Run this for randomly initialised starting points. This can take ~30 mins!\n","# np.random.seed(2022)\n","# init_params_list = np.random.uniform(1., 50., (10, 11))\n","# optim_res_list, log_lik_history_list  = optim_hyperparams_multiple_runs(\n","#     init_params_list, data_X, data_y, gp\n","# )"],"metadata":{"id":"EH_dr8VI0tzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Run this for starting points generated close to a set of optimal values.\n","np.random.seed(2022)\n","\n","# starting values are normally distributed around the optimal values\n","normal_noise = np.random.normal(size=(10, 11)) * np.array(\n","    [1., 1., 0.5, 1., 0.5, 0.1, 0.5, 0.1, 0.1, 0.5, 0.1],\n",")\n","init_params_list = np.array(\n","    [66., 67., 2.4, 90., 1.3, 0.66, 1.2, 0.78, 0.18, 1.6, 0.19],\n",") + normal_noise\n","\n","# Note: there might be warning messages generated by the optimiser\n","optim_res_list, log_lik_history_list  = optim_hyperparams_multiple_runs(\n","    init_params_list, data_X, data_y, gp, maxiter=5\n",")"],"metadata":{"id":"wHwCSCLhx70I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Exercise 8\n","1. Find the index of the largest log marginal likelihood from the list `log_lik_history_list`.\n","2. Find the final hyper-parameters from the run that gave the largest log marginal likelihood, and assign it to `final_params`. You may want to use the list `optim_res_list` containing all run histories."],"metadata":{"id":"_lXN7lMrkCsy"}},{"cell_type":"code","source":["### TODO ###\n","best_idx = None\n","best_optim_res = optim_res_list[best_idx]\n","best_log_lik = log_lik_history_list[best_idx]\n","\n","final_params = None\n","### END OF TODO ###"],"metadata":{"id":"JG8icqE8kBrv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TEST ###\n","assert len(final_params) == 11, \"Expect length of final_params to be 11\"\n","### END OF TEST ###"],"metadata":{"id":"r4A8diWAtypJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If the above cell did not return anything meaningful, use\n","```python\n","final_params = [66., 67., 2.4, 90., 1.3, 0.66, 1.2, 0.78, 0.18, 1.6, 0.19]\n","```"],"metadata":{"id":"Vzk7SuOpuWPa"}},{"cell_type":"code","source":["print(\"final hyper-params:\\n\", final_params)\n","print()\n","print(\"best log marginal likelihood:\", best_log_lik)"],"metadata":{"id":"PHFIzzvDFtGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# posterior predictions\n","pred_y = gp.predict(\n","    pred_X, data_X, data_y, size=100, theta=final_params[:-1], sigma=final_params[-1],\n",")\n","\n","plot_predicted_co2(data_X, data_y, pred_X, pred_y)"],"metadata":{"id":"ovRVkFohGUso"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Interpretation of Kernel Hyper-Parameters\n","Taking the final parameters\n","```python\n","final_params = [15.60917401 37.85666397 51.65420898 41.91108205  6.60621616  0.45302811\n","  0.54545549 21.92928839 27.86849259 20.36595657 -0.25296079]\n","\n","```\n","as an example, we see that \n","- The long-term trend has magnitude $\\theta_1 \\approx 16$ and a length-scale of $\\theta_2 \\approx 38$ years. \n","- The periodic contribution has magnitude $\\theta_3 \\approx 52$, decay-time $\\theta_4 \\approx 42$ years, and smoothness $\\theta_5 \\approx 7$. The long decay-time shows that the data has a component that is very close to a seasonal periodicity in the short term.\n","- The medium/short-term component has magnitude $\\theta_6 \\approx 0.5$ ppm, length-scale $\\theta_7 \\approx 0.5$ yearms and shape $\\theta_8 \\approx 22$. \n","- The correlated noise term has amplitude $\\theta_9 \\approx 28$ and lengt-scale $\\theta_{10} \\approx 20$, and the uncorrelated noise term has magnitude $\\theta_{11} \\approx 0.3$ (we take the absolute value due to unidentifiability: $\\theta_{11}^2 = (-\\theta_{11})^2$). The relatively small value of $\\theta_{11}$ suggests the uncorrelated noise level is small compared with the signal in the data, and the data have been very well explained by the model.\n","\n","As mentioned previously, this is only a local optimum and thus only one of many plausible sets of hyper-parameters that can explain the data. Each local optimum will correspond to a different interpretation of these parameters. You are encouraged to play around with the starting points and plot the resulting posterior draws to see how different hyper-parameters can affect the predictions."],"metadata":{"id":"2HVSXCUzTYWA"}},{"cell_type":"markdown","source":["# Conclusion\n","\n","In this notebook, we have seen how a GP regression model can be implemented from scratch using only very basic Python libraries. In practice, it is highly recommended to leverage many well-maintained Python frameworks available from the internet, which provide stable and efficient implementation of GPs and its variants; some examples are:\n","1. [scipy](https://scikit-learn.org/stable/modules/gaussian_process.html). They also provide a [tutorial](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html) on the Mauna Loa $\\textrm{CO}_2$ data over a different period of time. \n","2. [NumPyro](https://num.pyro.ai/en/stable/examples/gp.html)\n","3. [GPflow](https://www.gpflow.org/)"],"metadata":{"id":"DPeBNyIycBVO"}},{"cell_type":"markdown","source":["# References\n","\n","1. Rasmussen CE. Gaussian processes in machine learning. InSummer school on machine learning 2003 Feb 2 (pp. 63-71). Springer, Berlin, Heidelberg.\n","2. Keeling, C. D. and Whorf, T. P. (2004). Atmospheric CO2 Records from Sites in the SIO Air Sampling\n","Network. In Trends: A Compendium of Data on Global Change. Carbon Dioxide Information Analysis\n","Center, Oak Ridge National Laboratory, Oak Ridge, Tenn., U.S.A.\n","3. Global Monitoring Laboratory - Carbon Cycle Greenhouse Gases (last accessed: 28/12/2022). Available at: https://gml.noaa.gov/ccgg/trends/.\n"],"metadata":{"id":"zrdIr0reX0i0"}}]}