{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOVxzK3xz4N58pYUNDrXP3A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fully Connected Neural Networks\n","\n","<font color=\"green\"> \n","In this notebook, we will implement a Neural Network (NN) from scratch and apply it to classification tasks. The main objectives are:\n","1. Implementing gradient descent (GD) and stochastic gradient descent (SGD) from scratch.\n","2. Building a fully connected NN from scratch.\n","3. Building a fully connected NN using PyTorch APIs.\n","\n","</font>\n","\n","<font color=\"green\"> \n","Complete the exercises marked with \"Exercise\". Fill in the code blanks marked with \"TODO\".\n","\n","Do NOT change the cells marked with \"TEST\" -- they are for testing your completed code.\n","</font>"],"metadata":{"id":"9Ldbm169urVg"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","from sklearn.datasets import make_circles\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","import matplotlib.pyplot as plt"],"metadata":{"id":"_18frHjV0nRM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Gradient Descent\n","\n","In the first part of this notebook, we will implement gradient descent (GD) and stochastic gradient descent (SGD) from scratch. We will only focus on the parameter update of these algorithms and assume we are given an oracle that, given an input $x$ and a function $f$, returns the gradient of $f$ at $x$.\n","\n","Such an oracle can be achieved via [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (auto-diff, or AD). In brief, auto-diff is a set of techniques to evaluate the derivative of a given function. The derivatives computed by auto-diff is **exact** (unlike methods such as *finite difference* which approximates the derivative of a function), and the computational cost is almost the same as the function evaluation itself (no more than a small constant factor).\n","\n","We will use PyTorch for auto-diff. Details will follow later in this section.\n","\n","**Remarks:**\n","- In this case, since the objective is simply the least-square loss for a linear model, we can actually derive its gradient with respect to $\\beta$ **analytically** without resorting to auto-diff, as you have seen in Week 2's notebook. \n","- We use auto-diff here nonetheless as a demonstration of how this tool could be used to compute gradients in the general case, where the gradient of the objectives might be extremely tedious to derive analytically, as will be the case for neural network losses in the next part of this notebook."],"metadata":{"id":"mXWWgnnFFGy8"}},{"cell_type":"code","source":["class Convergence2DPlotting(object):\n","  \"\"\" Plotting utils for visualizing optimization paths on 2D functions. \"\"\"\n","\n","  def __init__(self, f, x_star):\n","    self.f = f\n","    self.fig, self.ax = plt.subplots()\n","    self.fig.set_size_inches(8.0, 8.0)\n","    self.ax.set_aspect(\"equal\")\n","\n","    # mark the optimal parameter x_star on plot.\n","    self.ax.scatter(x_star[0], x_star[1], color=\"red\", marker=\"x\", zorder=10)\n","\n","  def plot_iterates(self, iterates, color=\"C0\"):\n","    iterates = torch.tensor(iterates).squeeze()\n","    x, y = iterates[:,0], iterates[:, 1]\n","    self.ax.scatter(x,y,s=0)\n","    for i in range(len(x)-1):\n","      self.ax.annotate(\n","        \"\", \n","        xy=(x[i+1], y[i+1]), \n","        xytext=(x[i], y[i]),\n","        arrowprops={\"arrowstyle\": \"->\", \"color\":  color, \"lw\": 2},\n","      )\n","\n","  def plot_contours(self):\n","    x_min, x_max = self.ax.get_xlim()\n","    y_min, y_max = self.ax.get_ylim()\n","\n","    # generate the contours of f on the above computed range.\n","    n_points = 50\n","    x = torch.linspace(start=x_min, end=x_max, steps=n_points)\n","    y = torch.linspace(start=y_min, end=y_max, steps=n_points)\n","    x, y = torch.meshgrid(x, y)\n","    z = torch.zeros_like(x)\n","    for x_idx in range(n_points):\n","      for y_idx in range(n_points):\n","        input = torch.tensor([x[x_idx, y_idx], y[x_idx, y_idx]]).reshape(2,)\n","        z[x_idx, y_idx] = self.f(input)\n","    self.ax.contour(x,y,z, colors=\"k\")"],"metadata":{"id":"3bFFOoqRFBC8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Synthetic Linear Regression Dataset\n","\n","We will consider the **linear regression** problem. Given features $x_1, \\ldots, x_n \\in \\mathbb{R}^d$ and responses $y_1, \\ldots, y_n \\in \\mathbb{R}$ for which\n","$$\n","  y_i = x_i^\\top \\beta^* + \\epsilon_i \\;,\n","$$\n","where $\\epsilon_i \\sim \\mathcal{N}(0, 1)$ i.i.d., and $\\beta^* \\in \\mathbb{R}^d$ is the ground-truth parameter vector. **We aim to use GD/SGD to solve the least-square objective**\n","$$\n","  \\hat{\\beta}\n","  := \\arg\\min_{\\beta \\in \\mathbb{R}^d} \\sum_{i=1}^n (y_i - x_i^\\top \\beta)^2\n","  \\;.\n","$$\n","\n","We first define the MSE loss."],"metadata":{"id":"MkU9mHdEv4rC"}},{"cell_type":"code","source":["def mse(input, target):\n","  \"\"\"\n","  Compute the MSE:\n","    MSE(\\hat{y}, y) = \\sum_{i=1}^n (\\hat{y}_i - y_i)^2.\n","\n","  Args:\n","    input: (n,) A tensor of predicted responses.\n","    Target: (n,) A tensor of true responses. \n","  \n","  Returns:\n","    res: The MSE loss.\n","  \"\"\"\n","  res = torch.mean((input - target)**2)\n","  return res"],"metadata":{"id":"pfCalLvcwCmD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We then define a class that generates synthetic data according to the linear regression model described above. Note that the `eval_and_compute_loss` method is the function that we will minimize."],"metadata":{"id":"GI8mvO7a5Wae"}},{"cell_type":"code","source":["class LinearModel(object):\n","  \"\"\"\n","  Generates a linear regression dataset.\n","  \"\"\"\n","\n","  def __init__(self, n, d, seed: int=0):\n","    # set seed for reproducibility\n","    torch.manual_seed(seed)\n","\n","    # 1. generate the covariates matrix.\n","    X = torch.normal(0, 1, size=(n, d)) # Identity covariance component.\n","    X /= torch.sqrt(torch.sum(X**2, axis=1)).reshape(-1,1) # Normalize rows of X.\n","    # 2. sample some ground truth parameter w*.\n","    w_star = torch.normal(0, 1, size=(d,))\n","    # 3. normalize the ground truth parameter to have norm 5 (for 2d plots).\n","    w_star /= torch.sqrt(torch.sum(w_star**2)) / 5.0\n","    # 4. generate observations.\n","    y = self.eval(X, w_star) + 0.1 * torch.normal(0, 1, size=(n,))\n","    \n","    self.X = X              # features\n","    self.y = y              # responses\n","    self.w_star = w_star    # true parameters\n","\n","  def __getitem__(self, index):\n","    return self.X[index], self.y[index]\n","  \n","  def __len__(self):\n","    return self.n\n","\n","  def eval(self, X, w):\n","    \"\"\"\n","    Evaluate the linear model given feature matrix X and parameters w.\n","\n","    Args:\n","      X: (n, d) A feature matrix where each row is a feature vector.\n","      w: (d,) A vector of parameters.\n","\n","    Returns:\n","      (d,) A vector where the i-th entry is x_i^T w.\n","    \"\"\"\n","    return torch.matmul(X, w)\n","\n","  def eval_and_compute_loss(self, params):\n","    \"\"\"\n","    Evaluate the linear model given parameters w, and compute the loss. \n","\n","    Args:\n","      X: (n, d) A feature matrix where each row is a feature vector.\n","      w: (d,) A vector of parameters.\n","\n","    Returns:\n","      MSE(Xw, y)\n","    \"\"\"\n","    preds = self.eval(self.X, params)\n","    loss = mse(preds, self.y)\n","    return loss"],"metadata":{"id":"6MXr31PWoVK3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We generate 100 data points with dimension 2."],"metadata":{"id":"67-rJVM66Hjc"}},{"cell_type":"code","source":["lm = LinearModel(100, 2)"],"metadata":{"id":"v4CZ45NdvZqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"X:\", lm.X.shape, \"y:\", lm.y.shape)"],"metadata":{"id":"NWAevQPOmhhv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Expected output:\n","```python\n","X: torch.Size([100, 2]) y: torch.Size([100])\n","```"],"metadata":{"id":"JlfvXDbCdJYm"}},{"cell_type":"markdown","source":["## Gradient Descent\n","Recall that the GD algorithm proceeds as follows: Given a differentiable function $f: \\mathbb{R}^d \\to \\mathbb{R}$, some starting point $\\beta_0 \\in \\mathbb{R}^d$, and step-sizes $\\epsilon_t > 0$,\n","1. For $t = 1, 2, 3, \\ldots, $\n","  - $\\beta_t = \\beta_{t - 1} - \\epsilon_t \\nabla f(\\beta_{t-1})$.\n","\n","For simplicity, we will use a **fixed** step-size $\\epsilon_t \\equiv \\epsilon$ for some $\\epsilon > 0$, although it is not difficult to generalise to the variable step-sizes case.\n","\n","We will subclass the [`torch.optim.Optimizer`](https://pytorch.org/docs/stable/optim.html) module. This is a built-in module from PyTorch that allows custom optimisation algorithm to be implemented in a way that is compatible with the PyTorch synmantics. At its core are the following two attributes:\n","1. `parameters`: Variables to be optimised with respect to. It has to be an **iterable (e.g., list)** of `torch.tensor` that **requires gradient computation**.\n","2. `defaults`: A dictionary of any hyper-parameters for the optimisation algorithm.\n","\n","The parameters and hyper-parameters are accessible from the class attribute `param_groups`, which is a **list of parameter groups**, each being a **dictionary** with keys \n","- `params` (for the parameters), and \n","- the keys of any hyper-parameters in `defaults`. \n","\n","In our example, we will always have **a single parameter group**, although for the general case one might want to have more than one parameter group (e.g., one for each layer of a neural network, each with a different learning rate)."],"metadata":{"id":"P0cTN-gZoSd1"}},{"cell_type":"markdown","source":["#### Exercise 1\n","Implement the Gradient Descent algorithm. In the `step` method,\n","\n","1. Get the gradient evaluate at the parameters by `params.grad.data`.\n","2. Call [`params.data.add_`](https://pytorch.org/docs/stable/generated/torch.Tensor.add_.html) to update the parameters. You should provide the appropriate inputs to the fuction `_add` (remember that we wish to **minimise** a function).\n","\n","  <details><summary>Hint 1</summary>\n","  <p>\n","\n","  Given tensors `x`, `y` and `epsilon`, then `x._add(y, epsilon)` computes `x + epsilon*y`.\n","\n","  </p>\n","  </details>\n","\n","  <details><summary>Hint 2</summary>\n","  <p>\n","\n","  `_add` is an **in-place** operation, meaning that it modifies a given tensor directly without making a copy, so in particular no value is returned. \n","\n","  </p>\n","  </details>\n","\n","In the `optimize` method, \n","3. By studying the example in the [official documentation](https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step), take a single optimisation step.\n"],"metadata":{"id":"Odabu2vm8hV0"}},{"cell_type":"code","source":["class GD(torch.optim.Optimizer):\n","  def __init__(self, parameters, lr: float=1e-3):\n","    defaults = {\"lr\": lr}                  # set hyper-param for GD \n","    super().__init__(parameters, defaults) # pass args to parent class\n","\n","  def step(self, closure=None):\n","    \"\"\"\n","    Performs a single optimisation step.\n","    \"\"\"\n","    param_group = self.param_groups[0]\n","    params_list = param_group[\"params\"]\n","\n","    # loop through all parameters in the first parameter group\n","    for params in params_list:\n","      ### TODO ###\n","      # 1. get gradient\n","      grad = None\n","      # 2. update gradient\n","      params.data.add_(None)\n","\n","      ### END OF TODO ###\n","\n","  def optimize(self, f, niterations):\n","    \"\"\"\n","    Performs the Gradient Descent algorithm to find the \n","    minimiser to f.\n","\n","    Args:\n","      f: A function to be minimised.\n","      niterations: Number of times to perform the optimisation algorithm.\n","\n","    Returns:\n","      iterates: A list of w values at every iteration.\n","    \"\"\"\n","    w = self.param_groups[0][\"params\"][0]\n","\n","    iterates = []\n","    iterates.append(w.clone().detach().numpy().reshape(-1, 1))\n","    for _ in range(niterations):\n","      ### TODO ###\n","      # 3.1. reset gradients to zero \n","      self.zero_grad()\n","      # 3.2. compute loss\n","      loss = None\n","      # 3.3. compute gradient\n","      None\n","      # 3.4. update parameters\n","      None\n","      ### END OF TODO ###\n","\n","      # log intermediate values\n","      iterates.append(w.clone().detach().numpy().reshape(-1, 1))\n","\n","    return iterates"],"metadata":{"id":"Ve_khUS9FAn7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Auto-diff using PyTorch:**\n","\n","Note how we used PyTorch semantics to compute the gradient and update the parameters:\n","1. `zero_grad()` clears any gradient values previously stored in the optimizer to 0.\n","2. We then evaluate the function (loss). Any operations here will be logged for gradient computation later.\n","3. `backward()` computes the gradient using auto-diff. The \"backward\" refers to the algorithm used for computing the gradient, i.e., [back-propagation](https://en.wikipedia.org/wiki/Backpropagation).\n","4. `step()` performs a single optimisation step, in this case one GD update.\n","\n","Also note that the method `optimize` is merely a utility function for our exercise and is not required when building a custom optimiser with PyTorch --- only the `__init__` and `step` methods are always necessary. "],"metadata":{"id":"u1dEPCIxEklo"}},{"cell_type":"markdown","source":["We now initialise the optimiser and a starting point and performs GD to find the minimum. Note that when initiating `w`, we must set `requires_grad=True` to ask PyTorch log any operations performed with `w`. Otherwise no gradient information will be stored. "],"metadata":{"id":"K5H1KKTeGTFs"}},{"cell_type":"code","source":["w = torch.tensor([1., 1.], requires_grad=True)  # initial point\n","optimizer = GD([w], lr=0.1)                     # optimiser\n","\n","iterates_gd = optimizer.optimize(\n","    f=lm.eval_and_compute_loss,\n","    niterations=50,\n",")"],"metadata":{"id":"D88P3a9KwSCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gd_plotting = Convergence2DPlotting(\n","    f=lm.eval_and_compute_loss, x_star=lm.w_star,\n",")\n","gd_plotting.plot_iterates(iterates_gd)\n","gd_plotting.plot_contours()"],"metadata":{"id":"bihDCc7JyY7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Exercise 2\n","Fix `niterations` and vary the learning rate in the GD optimiser, and comment on your observations."],"metadata":{"id":"uDnKasfDpma2"}},{"cell_type":"markdown","source":["<details><summary>Solution</summary>\n","<p>\n","\n","When the learning rate is overly small (e.g., 0.01), each update will be tiny and it will require a large number of steps to converge.\n","\n","When the learning rate is large (e.g., 1.5), it might take fewer steps to reach to a region near the minimum, but it will then fluctuate around the minimum. \n","\n","An even larger learning rate (e.g., 2) can even causes divergence. \n","\n","**Bonus:** One can mathematically prove the number of GD steps needed to reach a given precision (see, e.g., Chapter 9.3.1 of [4]). Such analysis is called the **convergence analysis** of an optimisation algorithm.\n","\n","</p>\n","</details>\n"],"metadata":{"id":"J_pNk8AeI0d6"}},{"cell_type":"code","source":["# START YOUR CODE HERE"],"metadata":{"id":"SglDE_I3Idsq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stochastic Gradient Descent\n","\n","Often in machine learning the function $f$ that we wish to be minimised has the decomposition\n","$$\n","  f(\\beta) = \\sum_{i=1}^n f_i(\\beta) \\;,\n","$$\n","for some functions $f_i$. Examples include\n","- MSE: $l(\\hat{y}, y) = \\sum_{i=1}^n (\\hat{y}_i - y_i)^2$.\n","- Negative log-likelihood: $l(x) = - \\sum_{i=1}^n \\log p(x_i)$, where $p$ is a probability density function.\n","\n","In this case, if a single evaluation of $f$ is computationally costly (e.g., when $n$ is too large), then minimising $f$ via GD would become prohibitive, as GD usually requires evaluating $f$ by a large amount of times.\n","\n","Stochastic Gradient Descent (SGD) serves as a computationally cheap alternative in this case. Given an initial point $x_0$, SGD proceeds as:\n","1. For $t = 1, 2, \\ldots, T$,\n","  1. Randomly permute data points $1, \\ldots, n$.\n","  2. For $i = 1, 2, \\ldots, n$:\n","    - $\\beta_t = \\beta_{t-1} - \\epsilon_t \\nabla f_i(\\beta_{t-1})$.\n","\n","The intuition behind using $\\nabla f_i(\\beta_{t-1})$ to update the parameter is that it can be seen as a **Monte Carlo estimate** of the \"true\" gradient $\\nabla f(\\beta) = \\sum_{i=1}^n \\nabla f_i(\\beta)$ using a single Monte Carlo sample. \n","\n","\"Stochastic\" refers to the stochasticity in the radom permutation at each iteration. Compared with GD, the inner loop uses the gradient evaluated with **a single data point** to update the parameters. This means $T$ can be chosen to be relatively small to reduce the computationally burden.\n","\n","In machine learning terminologies, each outer loop (one iteration over the full dataset) is known as an **epoch**, and $T$ is called the **number of epochs**.\n"],"metadata":{"id":"owZEBXCToVSG"}},{"cell_type":"markdown","source":["**Data loaders**\n","To implement the SGD algorithm, we need to evaluate the objective function at every data point. To do so, we use [`torch.utils.data.DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), a tool that is able to partition a given dataset into subsets of a given size (called **batch size**).\n","\n","The input to dataloader can be of different types. One option is to provide a list where each element is a $(x_i, y_i)$ tuple.\n","\n","We also set `batch_size=1`, so that each time the dataloader returns a single pair of feature and response."],"metadata":{"id":"OD9QfseKSNSn"}},{"cell_type":"code","source":["data_sgd = torch.utils.data.DataLoader(\n","  dataset=[(lm.X[i], lm.y[i]) for i in range(lm.X.shape[0])], \n","  batch_size=1,\n",")"],"metadata":{"id":"BdHa8sbHSHnJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(data_sgd))"],"metadata":{"id":"E9PFOoWmTzBC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Expected output:\n","```python\n","[tensor([[-0.6988, -0.7153]]), tensor([0.1230])]\n","\n","```"],"metadata":{"id":"Rc5X_OYCVCEu"}},{"cell_type":"markdown","source":["#### Exercise 3\n","\n","Implement the SGD algorithm by completing the `optimize` method below.\n","1. Compute the model predictions using `model_eval`.\n","2. Compute the loss using the loss function `loss_fn`.\n","3. Back-propagate to update the parameters."],"metadata":{"id":"WEWLFQxCRw7R"}},{"cell_type":"code","source":["class SGD(GD):\n","  def __init__(self, parameters, lr: float=1e-3):\n","    super().__init__(parameters, lr)\n","\n","  def optimize(self, model_eval, loss_fn, niterations, data):\n","    \"\"\"\n","    Performs the Gradient Descent algorithm to find the \n","    minimiser to f.\n","\n","    Args:\n","      f: A function to be minimised.\n","      niterations: Number of times to perform the optimisation algorithm.\n","\n","    Returns:\n","      iterates: A list of w values at every iteration.\n","    \"\"\"\n","    w = self.param_groups[0][\"params\"][0]\n","\n","    iterates = []\n","    iterates.append(w.clone().detach().numpy().reshape(-1, 1))\n","    for _ in range(niterations):\n","      for X, y in data:\n","        self.zero_grad()\n","        ### TODO ###\n","        # compute model predictions\n","        preds = None\n","        # compute loss\n","        loss = None\n","        # back-propagate and update parameters\n","        None\n","        None\n","        ### END OF TODO ###\n","\n","        iterates.append(w.clone().detach().numpy().reshape(-1, 1))\n","\n","    return iterates"],"metadata":{"id":"jEtPucHSfW6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w = torch.tensor([1., 1.], requires_grad=True)\n","optimizer = SGD([w], lr=0.1)\n","\n","iterates_sgd = optimizer.optimize(\n","  model_eval=lm.eval,\n","  loss_fn=mse,\n","  niterations=5,\n","  data=data_sgd,\n",")"],"metadata":{"id":"WeiOGBkYgtcn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gd_plotting = Convergence2DPlotting(\n","    f=lm.eval_and_compute_loss, x_star=lm.w_star,\n",")\n","gd_plotting.plot_iterates(iterates_gd)\n","gd_plotting.plot_iterates(iterates_sgd, color=\"C1\")\n","gd_plotting.plot_contours()"],"metadata":{"id":"gDLscfrri6iN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Exercise 4\n","Comment on how the trace plot of SGD compares with that of GD."],"metadata":{"id":"KGrVmUNlWTiu"}},{"cell_type":"markdown","source":["<details><summary>Solution</summary>\n","<p>\n","  The trace plot of SGD is more noisy compared with that of GD. This is because each update step of SGD uses an estimate of the gradient, whereas GD uses the true gradient, which is the direction of **steepest descent**. As a result, SGD can take more iterations to converge, and the path fluctuates around the minimum. \n","\n","  **Bonus**: This noisy behaviour can sometimes be desirable, as it may help the optimisation algorithm to escape possible **local** minima of the objective function.\n","</p>\n","</details>"],"metadata":{"id":"CBQvJA43aGdl"}},{"cell_type":"markdown","source":["## Minibatch Gradient Descent\n","\n","SGD uses a single data point as an estimate of the gradient descent to save computational cost. However, estimates based on a single data point might be too crude, which would result in a slow convergence.\n","\n","The **Minibatch Gradient Descent** serves as a \"middle ground\" between GD and SGD, where $m$ data points ($1 \\leq m \\leq n$) are used at each step to estimate the gradient. It proceeds as follows:\n","1. For $t = 1, 2, \\ldots, T$,\n","  1. Randomly permute data points $1, \\ldots, n$.\n","  2. For $j = 1, 2, \\ldots, \\lfloor n/m \\rfloor$:\n","    - $\\beta_t = \\beta_{t-1} - \\frac{\\epsilon_t}{m} \\sum_{i=1 + m(j-1)}^{\\max(1+mj, n)} \\nabla f_i(\\beta_{t-1})$.\n","\n","The batch size $m$ trades-off the computational cost and convergence rate:\n","1. The larget the batch size, the faster the convergence rate due to more accurate gradient estimation, but the computational cost would be higher. It reduces to GD when $m = n$.\n","2. The smaller the batch size, the more computational (and memory) efficient, but the convergence would be slower. It reduces to SGD when $m = 1$.\n","\n","There is no best choice of $m$. As a rule of thumb, it is often chosen to be **multiples of 2** (32, 64, 128, 256, 512) for programming efficiency reasons when working with CPUs/GPUs.\n","\n"],"metadata":{"id":"3Fe_4aUJfXSJ"}},{"cell_type":"markdown","source":["We can use our `SGD` class to perform mini-batch GD by simply providing a dataloader that has `batch_size` greater than 1."],"metadata":{"id":"7tL-dFbUbrrq"}},{"cell_type":"code","source":["data_mbgd = torch.utils.data.DataLoader(\n","  dataset=[(lm.X[i], lm.y[i]) for i in range(lm.X.shape[0])], \n","  batch_size=5,\n",")\n","niterations = 10\n","\n","w = torch.tensor([1., 1.], requires_grad=True)\n","optimizer = SGD([w], lr=0.1)\n","iterates_mbgd = optimizer.optimize(\n","  model_eval=lm.eval,\n","  loss_fn=mse,\n","  niterations=niterations,\n","  data=data_mbgd,\n",")"],"metadata":{"id":"heWAmkF7jUXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gd_plotting = Convergence2DPlotting(\n","    f=lm.eval_and_compute_loss, x_star=lm.w_star,\n",")\n","gd_plotting.plot_iterates(iterates_gd)\n","gd_plotting.plot_iterates(iterates_sgd, color=\"C1\")\n","gd_plotting.plot_iterates(iterates_mbgd, color=\"C2\")\n","gd_plotting.plot_contours()"],"metadata":{"id":"qVgobcoQka1f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fully Connected Neural Networks from Scratch\n","\n","We will first build a feedforward NN without using the layer classes from `torch.nn`, and use it to classify images of clothes.\n"],"metadata":{"id":"sFUpKQ8uyg44"}},{"cell_type":"markdown","source":["## Fashion-MNIST Data\n","\n","The [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset contains images of clothes, where each example is a $28 \\times 28$ grayscale image that is associated with one of 10 possible labels. It is divided into a training set of 60,000 examples and a test set of 10,000 examples, where each class has an equal number of training and testing examples. Fashin-MNIST is often used as a more difficult alternative to the celebrated [MNIST](http://yann.lecun.com/exdb/mnist/)."],"metadata":{"id":"IclggvPGYsQt"}},{"cell_type":"code","source":["# set seed for reproducibility\n","SEED = 2023\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"],"metadata":{"id":"p0QXqSkI215E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can load the Fashion-MNIST dataset from the [`torchvision.datasets`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) API."],"metadata":{"id":"9auLFuJE78u3"}},{"cell_type":"code","source":["# download data\n","train_set = torchvision.datasets.FashionMNIST(\n","  root=\"./\", \n","  download=True, \n","  train=True,\n","  transform=transforms.Compose([transforms.ToTensor()]),\n",")\n","\n","test_set = torchvision.datasets.FashionMNIST(\n","  root=\"./\", \n","  download=True, \n","  train=False,\n","  transform=transforms.Compose([transforms.ToTensor()]),\n",")"],"metadata":{"id":"zW4Gmj4zWzrL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will only use a subset of the dataset that belong to certain classes. This is only to avoid an overly long training time."],"metadata":{"id":"AvU6Yy1G8lFS"}},{"cell_type":"code","source":["def preprocess(data, classes=[0, 1, 2]):\n","  \"\"\"\n","  Extracting only a subset of examples belonging\n","  to certain classes.\n","  \"\"\"\n","  data = [d for d in data if d[1] in classes]\n","  return data\n","\n","# keep only data with class labels in 0, 1, 2\n","train_set = preprocess(train_set)\n","test_set = preprocess(test_set)"],"metadata":{"id":"EjnSQem4Wzi0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now visualise some examples using `matplotlib.pyplot.imshow`. The labels are encoded as numerics $0, 1, 2, \\ldots, 9$. We rename them in the plot to the corresponding clothing types."],"metadata":{"id":"SZ8aJoW39FWi"}},{"cell_type":"code","source":["labels_map = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}\n","\n","def visualise(data, cols=3, rows=3):\n","  \"\"\"\n","  Utility function for plotting images with labels.\n","  \"\"\"\n","  fig = plt.figure(figsize=(8, 8))\n","  for i in range(cols * rows):\n","      img, label = data[i]\n","      fig.add_subplot(rows, cols, i+1)\n","      plt.title(labels_map[label])\n","      plt.axis(\"off\")\n","      plt.imshow(img.squeeze(), cmap=\"gray\")\n","  \n","  plt.tight_layout()\n","  plt.show()\n","\n","def visualise_pred(data, model, cols=3, rows=3):\n","  \"\"\"\n","  Utility function for plotting images with true\n","  and predicted labels.\n","  \"\"\"\n","  fig = plt.figure(figsize=(8, 8))\n","  for i in range(cols * rows):\n","    img, label = data[i]\n","    \n","    # true label\n","    label_text = labels_map[label]\n","\n","    # predicted label\n","    pred_probs = model(img)\n","    pred_label = torch.argmax(pred_probs, dim=-1).item()\n","    pred_label_text = labels_map[pred_label]\n","\n","    # different colors for wrong/correct predictions\n","    color = \"green\" if pred_label_text == label_text else \"red\"\n","\n","    # show image\n","    fig.add_subplot(rows, cols, i+1)\n","    plt.title(f\"True: {label_text}\\nPred: {pred_label_text}\", color=color)\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","  \n","  plt.tight_layout()\n","  plt.show()"],"metadata":{"id":"pMmAm_p-PXuS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset = [train_set[i] for i in torch.randint(len(train_set), size=(9,))]\n","visualise(subset)"],"metadata":{"id":"8VRha2vXPX5a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use mini-batch training. The `torch.utils.data.DataLoader` is a helpful tool for partitioning our training dataset to mini-batches."],"metadata":{"id":"sR50ATYK_N4e"}},{"cell_type":"code","source":["# use data loader\n","train_set = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)"],"metadata":{"id":"2jPGaaWFbx-U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Elements of a NN Classifier\n","\n","The NN model that we will build will have the following architecture:\n","<figure class=\"image\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1dGrZ1F_4_j6fxk5_TVoe7IUylENWb8pa\" alt=\"{{ include.description }}\" width=\"500\">\n","</figure>\n","\n","In words, an input data will be passed through:\n","1. **Flatten layer**: A layer that reshapes the input to a vector in $784$ dim.\n","2. **Hidden layer 1**: A fully connected linear layer with 256 nodes, followed by a ReLU activation function.\n","3. **Hidden layer 2**: A second fully connected linear layer with 256 nodes, followed by a ReLU activation function.\n","4. **Output layer**: A fully connected linear layer (with on activations).\n","\n","**Remarks:**\n","- Since the data input has shape `(28, 28)`, we first **flatten** it into a column vector of dimension $28 \\times 28 = 784$.\n","- The two hidden layers are fully connected linear layers.\n","- The output from each hidden layer is passsed through a **ReLU activation function**. \n","- The output will be a column vector $x_\\textrm{out}$ of dimension equal to the number of classes in the data. We will use $x_\\textrm{out}$ to model the **log predicted probability**, so each entry in $x_\\textrm{out}$ can in general take values in $\\mathbb{R}$. An alternative approach is to model instead the **predicted probability** by using an extra transformation (e.g., softmax) to enforce the output vector has non-negative entries that sum to 1.\n","\n","\n","\n","\n","**Further remarks:**\n","- Accuracy might not be a suitable metric if the data is **imbalanced**, meaning that the number of data points belonging to one class is vastly different from the others. However, since for Fashion-MNIST there is an equal number of examples in each class, this is not an issue."],"metadata":{"id":"8TT8D-45YWPf"}},{"cell_type":"markdown","source":["**To summarise, we will need the following:**\n","1. A fully connected linear layer, akin to [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n","2. A ReLU activation function, akin to [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).\n","3. [Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss function for model training.\n","4. [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) for model evaluation."],"metadata":{"id":"pzmi6GbmYaGw"}},{"cell_type":"markdown","source":["### Fully Connected Layers\n","\n","We will define a `Linear` layer class that inherits from the `torch.nn.Module` class from PyTorch. This allows us to build custom layer classes that can be used similarly as the built-in layer classes from `torch.nn`, so that we can leverage PyTorch's auto-differentiation to train our model.\n","\n","From the documentation of [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), there are several elements that we need to specify:\n","- `in_features`: shape of input,\n","- `out_features`: shape of output,\n","- `bias`: whether or not to include the bias term.\n","\n","The linear layer will then initialise a weight matrix and a bias vector of appropriate shapes, so that, when given an input of shape `(n, in_features)`, it returns an output of shape `(n, out_features)`.\n","\n","We will initialise these parameters from a standard Gaussian distribution. Initialisation of NN parameters can have significant impact on the model performance and convergence [1], and other initialisation strategies with better empirical performance exist (e.g., Kaiming initialisation [2], Xavier initialisation [3]). We do not dive deep into this topic in this notebook.\n","\n","Also, the class needs to hold the weights and bias as [`torch.nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) so that it can be trained using auto-differentiation."],"metadata":{"id":"LTpDr87Li4Di"}},{"cell_type":"markdown","source":["#### Exercise 5\n","Complete the `Linear` class below. In the `__init__` method,\n","1. Initialise the weights and bias terms with **appropriate shapes**, where each entry is drawn independently from a **standard Gaussian distribution**. \n","  - **Bonus:** If you feel ambitious, implement the [Kaiming uniform](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_) initialisation explained in the link. This is the initialisation used in `torch.nn.Linear`.\n","2. Pass (separately) the weights and bias into `torch.nn.Parameter`. This will register them as model parameters, which is needed for the backpropagation later on.\n","3. Set the weights and bias as attributes of the class.\n","\n","In the `forward` method,\n","4. Complete the operation by multiplying the input with the weigt matrix, and adding the bias term to the product."],"metadata":{"id":"sWFnMZ2rl-5n"}},{"cell_type":"code","source":["class Linear(nn.Module):\n","  def __init__(self, in_features, out_features, bias=True):\n","    super().__init__()\n","    self.in_features = in_features\n","    self.out_features = out_features\n","    self.bias = bias\n","\n","    ### TODO ###\n","    # initialise weights and biases\n","    self.weight = None\n","    self.bias = None\n","    ### END OF TODO ###\n","\n","  def forward(self, input):\n","    \"\"\"\n","    Args:\n","      input: (n, d_in)\n","\n","    Returns:\n","      output: (n, d_out)\n","    \"\"\"\n","    dim = input.shape[-1]\n","    if dim != self.in_features:\n","      raise RuntimeError(f\"Wrong input feature shape. Expect {self.in_features} but received {dim}.\")\n","    \n","    ### TODO ###\n","    output = None\n","    ### END OF TODO ###\n","\n","    return output"],"metadata":{"id":"j77MLkAw4DcN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TEST ###\n","_linear = Linear(in_features=3, out_features=8)\n","_xx = torch.randn((10, 3))\n","_linear(_xx), _linear(_xx).shape\n","### END OF TEST ###"],"metadata":{"id":"xSQCyjj94DyW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Exercise 6\n","Complete the `ReLU` class below.\n","1. Given an input $x$, it computes **component-wise** $\\textrm{ReLU}(x) = \\max(0, x)$.\n","2. You will find the [`torch.clamp`](https://pytorch.org/docs/stable/generated/torch.clamp.html) function helpful.\n"],"metadata":{"id":"6fmkB973ntGC"}},{"cell_type":"code","source":["class ReLU(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","  def forward(self, input):\n","    \"\"\"\n","    Apply component-wise the ReLU function to the input.\n","    \"\"\"\n","    ### TODO ###\n","    output = None\n","    ### END OF TODO ###\n","\n","    return output"],"metadata":{"id":"LgBUlg0ahnP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TEST ###\n","_x = torch.tensor([0.5, 0., -0.5])\n","_y1 = ReLU()(_x)\n","_y2 = nn.ReLU()(_x)\n","assert torch.allclose(_y1, _y2), \"ReLU implementation does not match built-in function\"\n","### END OF TEST ###"],"metadata":{"id":"g8h3tEQch-w1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we define a flatten layer that converts a rank-2 tensor to a rank-1 tensor.\n","\n","#### Exercise 7\n","Complete the class below. Given an input of rank 2, the `forward` method should output the input tensor reshaped to rank 1 (i.e., a vector)."],"metadata":{"id":"ArOPjatFOeTj"}},{"cell_type":"code","source":["class Flatten(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","  def forward(self, input):\n","    \"\"\"\n","    Args:\n","      input: (..., d1, d2) A tensor of arbitrary shape.\n","\n","    Returns:\n","      output: The input tensor reshaped to rank 1.\n","    \"\"\"\n","    ### TODO ###\n","    output = None\n","    ### END OF TODO ###\n","\n","    return output"],"metadata":{"id":"994hMHFrftjN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TEST ###\n","_x = torch.tensor([[1., 2.], [3., 4.], [5., 6.]])\n","_y1 = Flatten()(_x)\n","_y2 = nn.Flatten()(_x)\n","assert torch.allclose(_y1, _y2), \"Flatten implementation does not match built-in funciton.\"\n","### END OF TEST ###"],"metadata":{"id":"7buMmirXgp5L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loss Function\n","We will train the model using the [**cross-entropy loss**](https://en.wikipedia.org/wiki/Cross_entropy). Given a class label $y \\in \\mathcal{C}$ where $\\mathcal{C}$ denotes the set of class labels, and given $x_\\textrm{out} \\in \\mathbb{R}^{|\\mathcal{C}|}$, we define \n","\\begin{align}\n","  l(x_\\textrm{out}, y)\n","  = - \\log \\frac{ \\exp\\left( x^{(y)}_\\textrm{out} \\right) }{\\sum_{c \\in \\mathcal{C}} \\exp\\left( x^{(c)}_\\textrm{out} \\right)} \n","  \\;,\n","  \\tag{1}\n","\\end{align}\n","where $x^{(c)}_\\textrm{out}$ denotes the **$c$-th component** of $x_\\textrm{out}$ (**not** the $c$-th power). The **cross-entropy loss** is defined as the sum over all data points:\n","\\begin{align}\n","  \\mathcal{L}(\\mathbf{X}, \\mathbf{y})\n","  := \\sum_{i = 1}^N l(f(x_i), y) \\;,\n","  \\tag{2}\n","\\end{align}\n","where $f: \\mathbb{R}^d \\to \\mathbb{R}^{|\\mathcal{C}|}$ denotes the NN model, and $\\mathbf{X} := \\{ x_i \\}_{i=1}^N$ and $\\mathbf{y} = \\{ y_i \\}_{i=1}^N$ denote the features and labels of the training data, respectively.\n"],"metadata":{"id":"ax94d0pPDBgz"}},{"cell_type":"markdown","source":["#### Exercise 8\n","Complete the function below that computes the cross-entropy loss. It should \n","1. Take as its input:\n","  1. `input`: a tensor of shape `(n, nclass)`, where each row contains the predicted **log** probability for each class.\n","  2. `target`: a tensor of shape `(n,)`, where each entry contains the label.\n","2. Return the cross-entropy loss (2) **divided by $n$**.\n","\n","**Remarks:** Your code should be numerically stable. You will find `torch.logsumexp` useful."],"metadata":{"id":"MYrCfqkPP3UV"}},{"cell_type":"code","source":["def cross_entropy(input, target):\n","  \"\"\"\n","  Args:\n","    input: (n, nclass) A tensor where the (i, j)-th entry contains the predicted \n","      **log** probability that example i belongs to class j.\n","    target: (n,) A tensor the i-th each entry contains the label for example i.\n","  \n","  Returns:\n","    loss: The cross-entropy loss divided by n.\n","  \"\"\"\n","  n = input.shape[0]\n","  \n","  ### TODO ###\n","  loss = None\n","  ### END OF TODO ###\n","\n","  return loss\n"],"metadata":{"id":"MqVHjRlgAiqc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TEST ###\n","torch.manual_seed(SEED)\n","_x = torch.randn(3, 5)\n","_y = torch.randint(low=0, high=5, size=(3,))\n","\n","_loss = nn.CrossEntropyLoss()\n","_output1 = _loss(_x, _y)\n","_output2 = cross_entropy(_x, _y)\n","\n","assert torch.allclose(_output1, _output2), \"Cross-entropy does not match built-in function\"\n","### END OF TEST ###"],"metadata":{"id":"lam6e5d44D_Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation Metric\n","Finally, we will evaluate the model using the accuracy metric:\n","$$\n","  \\textrm{Accuracy}\n","  = \\frac{1}{N} \\sum_{i=1}^N 1{(y_i = \\hat{y}_i)} \\;,\n","$$\n","where $\\hat{y}_i$ denotes the **predicted** label for the $i$-th example."],"metadata":{"id":"vTZXsPAYJARr"}},{"cell_type":"markdown","source":["#### Exercise 9\n","Complete the function below that computes the accuracy metric."],"metadata":{"id":"5sJksr4sPxXL"}},{"cell_type":"code","source":["def accuracy(preds, target):\n","  \"\"\"\n","  Args:\n","    preds: (n, nclass) A tensor where the (i, j)-th entry contains the predicted \n","      **log** probability that example i belongs to class j.\n","    target: (n,) A tensor the i-th each entry contains the label for example i.\n","  \n","  Returns:\n","    acc: The accuracy metric.\n","  \"\"\"\n","  ### TODO ###\n","  acc = None\n","  ### END OF TODO ###\n","  \n","  return acc"],"metadata":{"id":"9v5NNhfkPDw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TEST ###\n","_preds = torch.tensor([[0.1, 0.2, 0.7], [0.8, 0.1, 0.1], [0.8, 0.1, 0.1], [0.2, 0.5, 0.3]])\n","_y = torch.tensor([0., 2., 0., 1.])\n","assert accuracy(_preds, _y) == 0.5, \"Accuracy does not match built-in funciton\"\n","### END OF TEST ###"],"metadata":{"id":"0r9vzUhvUGuX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build a NN Model\n","\n","We now have all elements needed to build a neural network. Let us now create a model with the architecture shown at the beginning of this section."],"metadata":{"id":"3F7MhHcLNcr4"}},{"cell_type":"markdown","source":["#### Exercise 10\n","1. In the `__init__` method, create\n","  1. A `Flatten` layer.\n","  2. A `ReLU` activation function.\n","  3. A `Linear` layer taking in input tensors of dimension `dim` and returning output tensors of dimension $256$ (hidden layer 1).\n","  4. A `Linear` layer taking in input tensors of dimension $256$ and returning output tensors of dimension $256$ (hidden layer 2).\n","  5. A `Linear` layer taking in input tensors of dimension $256$ and returning output tensors of dimension `nclass` (output layer).\n","\n","2. In the `forward` method, pass `input` through the layers and activation functions in the order\n","  1. Flatten layer.\n","  2. Hidden layer 1.\n","  3. ReLU activation function.\n","  4. Hidden layer 2.\n","  5. ReLU activation function.\n","  6. Output layer."],"metadata":{"id":"FCfnzB1FA2Ej"}},{"cell_type":"code","source":["class Classifier(nn.Module):\n","  def __init__(self, dim, nclass):\n","    super().__init__()\n","    ### TODO ###\n","    self.flatten = None\n","    self.relu = None\n","    self.l1 = None\n","    self.l2 = None\n","    self.output = None\n","    ### END OF TODO ###\n","\n","  def forward(self, input):\n","    ### TODO ###\n","    x = None\n","    x = None\n","    x = None\n","    x = None\n","    x = None\n","    output = None\n","    ### END OF TODO ###\n","\n","    return output"],"metadata":{"id":"M9zr74tLflP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dim, nclass = 784, 3"],"metadata":{"id":"NxU8XHVxdnnq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# instantiate model\n","model = Classifier(dim, nclass)"],"metadata":{"id":"Yy_SSAz1oXWn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can inspect the model performance **before any training**. Note that [`torch.no_grad`](https://pytorch.org/docs/stable/generated/torch.no_grad.html) tempararily **disables** all gradient computation, so that no operation within the `with torch.no_grad()` chunk would be logged by the gradient tracker in PyTorch. This is useful when we do not intend the operations to be included in back-propagation, e.g., during the evaluation stage of a model. "],"metadata":{"id":"gMDZYOlUEJ9x"}},{"cell_type":"code","source":["X_test = torch.stack([t[0] for t in test_set])\n","y_test = torch.tensor([t[1] for t in test_set])\n","print(\"test X:\", X_test.shape, \"test y:\", y_test.shape)"],"metadata":{"id":"Ireg-1wm47G_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  preds_test = model(X_test)\n","\n","print(\"accuracy before training\", accuracy(preds_test, y_test).item())"],"metadata":{"id":"bY-JZ01uoYPq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can randomly select some examples and compare their predicted and true labels. As expected, many predictions are wrong (marked in red)."],"metadata":{"id":"YZ69lYZcGsV3"}},{"cell_type":"code","source":["subset = [train_set.dataset[i] for i in torch.randint(len(train_set), size=(9,))]\n","visualise_pred(subset, model, cols=3, rows=3)"],"metadata":{"id":"-afwvvghEK-E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training and Evaluation\n","\n","We now define a utility function that allows us to train the NN model and compute the intermediate losses and accuracies."],"metadata":{"id":"BWPrDe_lCFbg"}},{"cell_type":"markdown","source":["#### Exercise 11\n","Complete the `train` function that trains a given PyTorch model using back-propagation. Within each loop,\n","\n","1. Pass the current batch of features `X` to the model to get the predicted log probabilities.\n","2. Compute the loss by passing the predicted log probabilities and the true labels to `loss_fn`.\n","3. Update the model parameters:\n","  - Reset the gradients in the optimizer.\n","  - Back-propagate to compute the gradients.\n","  - Perform one-step update to the model parameters.\n"],"metadata":{"id":"TZIYdBBy9WDJ"}},{"cell_type":"code","source":["def train(\n","  model: torch.nn.Module, \n","  nepochs: int, \n","  train_dataloader: torch.utils.data.DataLoader,\n","  optimizer: torch.optim.Optimizer, \n","  loss_fn: callable,\n","  X_test: torch.tensor,\n","  y_test: torch.tensor,\n","  print_every_epoch: int=10,\n","):\n","  \"\"\"\n","  Train a model using backpropagation based on PyTorch and evaluate loss and\n","  accuracy at each epoch.\n","\n","  Args:\n","    model: A NN model.\n","    \n","    nepochs: Number of training epochs.\n","    \n","    train_dataloader: A dataloader containing the training features and labels.\n","    \n","    optimizer: The optimisation algorithm used for the backpropagation.\n","\n","    loss_fn: The loss function used to train the model.\n","\n","    X_test: Test set features.\n","\n","    y_test: Test set labels.\n","\n","    print_every_epoch: Print progress messages once every print_every_epoch\n","      epochs.\n","\n","  Returns:\n","    res: A dictionary containing\n","      \"loss_train\": training loss\n","      \"loss_test\": test loss\n","      \"accuracy_train\": training accuracy\n","      \"accuracy_test\": test accuracy\n","  \"\"\"\n","  loss_train_list = []\n","  loss_test_list = []\n","  accuracy_train_list = []\n","  accuracy_test_list = []\n","  \n","  for epoch in range(nepochs):\n","    total_loss = 0\n","    total_correct = 0\n","    ntrain = 0\n","    \n","    # start timing\n","    start_time = time.time()\n","    for batch in train_dataloader:\n","      # get training features and labels in current batch\n","      X, y = batch\n","      y = y.to(torch.int64) # change data type\n","\n","      ### TODO ###\n","      # 1. get predictions\n","      preds = None \n","      # 2. compute loss\n","      loss = None\n","      # 3. back propagation\n","      None # reset grads to zero\n","      None # calculates gradients \n","      None # update parameters\n","\n","      ### END OF TODO ###\n","      \n","      # add minibatch loss to the total loss\n","      total_loss += loss.item()\n","\n","      # add number of correct predictions in minibatch\n","      # to the total count\n","      ntrain += X.shape[0]\n","      total_correct += accuracy(preds, y) * X.shape[0]\n","\n","    end_time = time.time() - start_time # end timing\n","    \n","    # log training loss\n","    loss_train_list.append(total_loss)\n","  \n","    # compute training accuracy\n","    accuracy_train = total_correct / ntrain\n","    accuracy_train_list.append(accuracy_train)\n","\n","    # test metrics\n","    with torch.no_grad():\n","      # test loss\n","      preds_test = model(X_test)\n","      loss_test = loss_fn(preds_test, y_test.to(torch.int64))\n","      loss_test_list.append(loss_test)\n","\n","      # test accuracy\n","      accuracy_test = accuracy(preds_test, y_test)\n","      accuracy_test_list.append(accuracy_test)\n","\n","    if epoch % print_every_epoch == 0:\n","      print(\n","        \"Epoch no.\", epoch+1, \"| total_loss: \", total_loss,\n","        \"| epoch_duration: \", round(end_time,2),\"sec\",\n","      )\n","\n","  res = {\n","      \"loss_train\": loss_train_list,\n","      \"loss_test\": loss_test_list,\n","      \"accuracy_train\": accuracy_train_list,\n","      \"accuracy_test\": accuracy_test_list,\n","  }\n","  return res"],"metadata":{"id":"PztiF57POlU4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nepochs = 20\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","\n","res = train(\n","  model=model, \n","  nepochs=nepochs, \n","  train_dataloader=train_set, \n","  optimizer=optimizer, \n","  loss_fn=cross_entropy,\n","  X_test=X_test,\n","  y_test=y_test,\n",")"],"metadata":{"id":"pCF2veRTbx3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_fmnist_results(nepochs, res):\n","  \"\"\"\n","  Utility function for plotting the training and testing loss and accuracy\n","  at each epoch.\n","  \"\"\"\n","  # plot loss\n","  figs, axes = plt.subplots(ncols=2, nrows=1)\n","  axes[0].plot(range(nepochs), res[\"loss_train\"], color=\"C0\", label=\"train\")\n","  axes[0].set_xlabel(\"Epochs\")\n","  axes[0].set_ylabel(\"Loss\")\n","  axes[0].legend()\n","  \n","  axes[1].plot(range(nepochs), res[\"loss_test\"], color=\"C1\", label=\"test\")\n","  axes[1].set_xlabel(\"Epochs\")\n","  axes[1].set_ylabel(\"Loss\")\n","  axes[1].legend()\n","  \n","  # plot accuracy\n","  fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(5, 3))\n","  ax.plot(range(nepochs), res[\"accuracy_train\"], label=\"train\")\n","  ax.plot(range(nepochs), res[\"accuracy_test\"], label=\"test\")\n","  ax.set_xlabel(\"Epochs\")\n","  ax.set_ylabel(\"Accuracy\")\n","  ax.legend()\n","\n","  plt.tight_layout()"],"metadata":{"id":"Xy3t3r4avmVy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_fmnist_results(nepochs, res)"],"metadata":{"id":"uhVzA5Zmw83O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  preds_test = model(X_test)\n","\n","print(\"accuracy after training\", accuracy(preds_test, y_test).item())"],"metadata":{"id":"ddRanC9T1ixp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualise_pred(subset, model, cols=3, rows=3)"],"metadata":{"id":"6RmTbhi601lW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Neural Networks using PyTorch APIs\n","\n","We have seen how to implement a neural network model from scratch using only basic PyTorch APIs. Not surprisingly, all the tools we have implemented so far (linear and flatten layers, ReLU activation, accuracy metric) can be accessed directly from PyTorch. Moreoever, PyTorch also provides a variety of other well-known layer types, activation functions, evaluation metrics etc. See their documentation for details:\n","- [Layers, activation and loss functions](https://pytorch.org/docs/stable/nn.html).\n","- [Evaluation metrics](https://pytorch.org/torcheval/stable/torcheval.metrics.html).\n","\n","We demonstrate here how to construct the same NN model using purely PyTorch APIs. The semantics are almost the same as before. "],"metadata":{"id":"d47W1VQ7FBr7"}},{"cell_type":"code","source":["class ClassifierTorch(nn.Module):\n","  def __init__(self, dim, nclass):\n","    super().__init__()\n","    self.flatten = nn.Flatten()\n","    self.l1 = nn.Linear(dim, 256)\n","    self.relu = nn.ReLU()\n","    self.l2 = nn.Linear(256, 256)\n","    self.output = nn.Linear(256, nclass)\n","\n","  def forward(self, input):\n","    x = self.flatten(input) # equivalently, x = x.view(x.size()[0], -1)\n","    x = self.l1(x)\n","    x = self.relu(x)\n","    x = self.l2(x)\n","    x = self.relu(x)\n","    output = self.output(x)\n","    return output"],"metadata":{"id":"IQK17bNUFBiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nepochs = 20\n","model_torch = ClassifierTorch(dim, nclass)\n","optimizer = torch.optim.SGD(model_torch.parameters(), lr=0.01)\n","loss_fn = nn.functional.cross_entropy"],"metadata":{"id":"m8VhhGHAH66Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  preds_test_torch = model_torch(X_test)\n","\n","print(\"accuracy before training\", accuracy(preds_test_torch, y_test).item())"],"metadata":{"id":"0f1E6cQA1tf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res_torch = train(\n","  model=model_torch, \n","  nepochs=nepochs,\n","  train_dataloader=train_set, \n","  optimizer=optimizer,\n","  loss_fn=loss_fn,\n","  X_test=X_test,\n","  y_test=y_test,\n",")"],"metadata":{"id":"rzu1Of2uICe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_fmnist_results(nepochs, res_torch)"],"metadata":{"id":"WE2OiB1dycbU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The starting point of the training loss is much lower than the one using our custom NN model. This could attribute to the naive initialisation we adopted. "],"metadata":{"id":"0onU_b8MK0jx"}},{"cell_type":"code","source":["with torch.no_grad():\n","  preds_test_torch = model_torch(X_test)\n","\n","print(\"accuracy after training\", accuracy(preds_test_torch, y_test).item())"],"metadata":{"id":"tYb4GloO14u5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualise_pred(subset, model_torch, cols=3, rows=3)"],"metadata":{"id":"yj5rGquz2rG4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# References\n","1. Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.\n","2. He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. InProceedings of the IEEE international conference on computer vision 2015 (pp. 1026-1034).\n","3. Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. InProceedings of the thirteenth international conference on artificial intelligence and statistics 2010 Mar 31 (pp. 249-256). JMLR Workshop and Conference Proceedings.\n","4. Boyd S, Boyd SP, Vandenberghe L. Convex optimization. Cambridge university press; 2004 Mar 8.\n","5. Sebastian Ruder (2020) An overview of gradient descent optimization algorithms, Sebastian Ruder. Sebastian Ruder. Available at: https://ruder.io/optimizing-gradient-descent/index.html#minibatchgradientdescent (Accessed: January 7, 2023)."],"metadata":{"id":"6rAJJI2PfO58"}}]}